# ğŸš€ TensorRT-LLM Deployment on B200 GPUs

<div align="center">

![NVIDIA](https://img.shields.io/badge/NVIDIA-AI-blue?style=for-the-badge&logo=nvidia)
![TensorRT](https://img.shields.io/badge/TensorRT--LLM-2.0-green?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python)
![Docker](https://img.shields.io/badge/Docker-Containers-blue?style=for-the-badge&logo=docker)

**âš¡ Production-Ready LLM Deployment on 8x NVIDIA B200 GPUs âš¡**

*Optimized for GPT-OSS-120B with 2-10x faster inference than PyTorch*

[ğŸ“– Quick Start](#-one-command-deployment-recommended) â€¢ [ğŸ”§ Manual Setup](#manual-deployment) â€¢ [ğŸ“Š Benchmarks](#benchmarking) â€¢ [ğŸ› Troubleshooting](#troubleshooting)

</div>

---

## ğŸ“‹ Table of Contents

- [ğŸš€ One-Command Deployment (Recommended)](#-one-command-deployment-recommended)
- [ğŸ”§ Manual Deployment](#manual-deployment)
- [âš™ï¸ Configuration](#configuration)
- [ğŸ—ï¸ Architecture](#architecture)
- [ğŸ“œ Scripts Overview](#scripts-overview)
- [ğŸ“ Directory Structure](#directory-structure)
- [ğŸ¤– Model Configuration](#model-configuration)
- [âš¡ Performance Tuning](#performance-tuning)
- [ğŸ“Š Benchmarking](#benchmarking)
- [ğŸ” Troubleshooting](#troubleshooting)
- [ğŸ’» API Usage Examples](#api-usage-examples)
- [ğŸ¤ Contributing](#contributing)

## Configuration

### Environment Setup
```bash
# 1. Copy and edit configuration
cp .env.example .env
# Edit with your NGC API key and preferences

# 2. Setup standard directory structure
bash scripts/setup/setup_directories.sh

# 3. Auto-download model (if configured)
bash scripts/download/download_model_auto.sh
```

### Key Configuration Options
- `NGC_API_KEY`: Your NVIDIA NGC API key
- `MODEL_NAME`: Model to deploy (default: nvidia/gpt-oss-120b)
- `FRAMEWORK`: tensorrt-llm or vllm
- `AUTO_DOWNLOAD_MODEL`: true/false for automatic Hugging Face downloads
- `PORT`: Server port (default: 8000)

### ğŸš€ One-Command Deployment (Recommended)

**âœ¨ The easiest way to deploy is using the interactive `main.sh` script:**

```bash
ğŸ¯ bash main.sh
```

**What it does:**
- ğŸ”§ **Guides** you through configuration setup
- ğŸ›ï¸ **Lets you choose** between TensorRT-LLM or vLLM
- âš™ï¸ **Handles all setup** steps automatically
- ğŸš€ **Starts your LLM server** with one command
- ğŸ§ª **Provides testing** and benchmarking options

### ğŸ® Interactive Menu Options

1. **ğŸš€ Full deployment** - Complete setup and deployment in one go
2. **ğŸ”§ Setup only** - Configure environment without starting server
3. **â–¶ï¸ Deploy only** - Start server using existing setup
4. **ğŸ§ª Test only** - Test a running server
5. **ğŸ“Š Benchmark only** - Run performance benchmarks

### ğŸ¯ Why Use main.sh?

- âœ… **Beginner-friendly** - No need to remember commands
- âœ… **Error handling** - Guides you through issues
- âœ… **Framework choice** - Easy switching between TensorRT-LLM and vLLM
- âœ… **Status updates** - Real-time progress with emojis
- âœ… **One command** to live LLM! ğŸ‰

### ğŸ”§ Manual Deployment

**If you prefer manual control, follow these steps:**

#### ğŸ“‹ Prerequisites
- ğŸ–¥ï¸ **Access** to compute node: `exp-blr-dgxb200-01`
- ğŸ”‘ **NGC API key** from https://ngc.nvidia.com/setup/api-key
- ğŸ” **SSH access** to the cluster

#### ğŸ¯ Choose Your Framework

**ğŸ† Option 1: TensorRT-LLM (Recommended - 2-10x faster)**
```bash
# Setup TensorRT-LLM container
bash deploy/02_setup_container.sh tensorrt-llm
```

**ğŸ”„ Option 2: vLLM (Easier setup, more flexible)**
```bash
# Setup vLLM container
bash deploy/02_setup_container.sh vllm
```

#### ğŸš€ Deploy GPT-OSS-120B

**â­ Option 1: Setup Both Frameworks (Recommended)**
```bash
# One-time setup for both containers
bash scripts/setup/setup_both_frameworks.sh
```

**ğŸ¯ Option 2: Setup Individual Frameworks**
```bash
# Setup only TensorRT-LLM
bash deploy/02_setup_container.sh tensorrt-llm

# Setup only vLLM
bash deploy/02_setup_container.sh vllm
```

#### â–¶ï¸ Run Frameworks One by One

```bash
# 1. SSH to compute node
ssh exp-blr-dgxb200-01

# 2. Verify hardware
bash deploy/01_verify_hardware.sh

# 3. Start container (auto-detects available container)
bash deploy/03_start_container.sh

# 4. Inside container - Choose your framework:
# For TensorRT-LLM (2-6 hours engine building)
bash /workspace/deploy/04_deploy_tensorrt_llm.sh

# For vLLM (much faster startup)
bash /workspace/deploy/04_deploy_vllm.sh
```

#### ğŸ”„ Switch Between Frameworks

**To switch between frameworks, exit the current container and start the other one:**

```bash
# Exit current container (type 'exit' or Ctrl+D)
exit

# Start the other container
bash deploy/03_start_container.sh tensorrt-llm  # or vllm-pytorch
```

## ğŸ—ï¸ Architecture

### ğŸ¯ Framework Options

#### ğŸ† TensorRT-LLM (Recommended)
- âš¡ **Performance:** 2-10x faster inference than PyTorch/vLLM
- ğŸ§  **Memory Efficiency:** Optimized KV cache management with PagedAttention
- ğŸ”— **Multi-GPU:** Native support for tensor parallelism across 8 GPUs
- ğŸ¢ **Production Ready:** NVIDIA's enterprise-grade inference framework
- â±ï¸ **Setup Time:** 2-6 hours for engine building, then very fast inference

#### ğŸ”„ vLLM
- ğŸ“ˆ **Performance:** Good performance with PagedAttention
- ğŸ›ï¸ **Flexibility:** Supports more model architectures and configurations
- ğŸš€ **Ease of Use:** Much faster setup and startup times
- ğŸŒ **Community:** Large open-source community and active development
- âš¡ **Setup Time:** Minutes to install and start serving

### ğŸ’» Hardware Configuration
- ğŸ® **GPUs:** 8x NVIDIA B200 (183GB HBM3e each)
- ğŸŒ **Interconnect:** NVSwitch for high-speed GPU-to-GPU communication
- ğŸ’¾ **Memory:** 1.4TB total VRAM
- ğŸ **CUDA:** Version 13.0
- ğŸ§ **OS:** Ubuntu 24.04

## ğŸ“œ Scripts Overview

| ğŸ¯ Script | ğŸ“ Purpose |
|-----------|------------|
| `main.sh` | ğŸ® **Interactive deployment script (recommended)** |
| `.env` | âš™ï¸ **Configuration file (created from .env.example)** |
| `.env.example` | ğŸ“‹ **Configuration template** |
| `scripts/setup/load_config.sh` | ğŸ”„ **Load .env configuration** |
| `scripts/setup/setup_directories.sh` | ğŸ“ **Create standard directory structure** |
| `scripts/download/download_model_auto.sh` | â¬‡ï¸ **Auto-download models from Hugging Face** |
| `scripts/setup/setup_both_frameworks.sh` | ğŸ”§ **Setup both TensorRT-LLM and vLLM containers** |
| `deploy/01_verify_hardware.sh` | âœ… **Verify 8x B200 GPUs and NVSwitch connectivity** |
| `deploy/02_setup_container.sh` | ğŸ³ **Setup individual Enroot container (TensorRT-LLM or vLLM)** |
| `deploy/03_start_container.sh` | â–¶ï¸ **Start container with GPU access and workspace mounting** |
| `deploy/04_deploy_tensorrt_llm.sh` | ğŸš€ **Build TensorRT engine and start OpenAI-compatible API server** |
| `deploy/04_deploy_vllm.sh` | âš¡ **Install vLLM and start OpenAI-compatible API server** |
| `scripts/test/05_test_server.sh` | ğŸ§ª **Comprehensive vLLM server testing** |
| `scripts/test/quick_test_server.sh` | âš¡ **Quick TensorRT-LLM server health check** |
| `scripts/benchmark/benchmark_tensorrt_llm.py` | ğŸ“Š **Comprehensive performance benchmarking** |
| `scripts/benchmark/install_benchmark_deps.sh` | ğŸ“¦ **Install benchmarking dependencies** |
| `scripts/benchmark/requirements_benchmark.txt` | ğŸ **Python dependencies for benchmarking** |
| `scripts/benchmark/BENCHMARK_README.md` | ğŸ“– **Detailed benchmarking guide** |

## ğŸ“ Directory Structure

**After running `setup_directories.sh`, you'll have this standard structure:**

```
workspace/
â”œâ”€â”€ âš™ï¸ .env                    # Configuration file
â”œâ”€â”€ ğŸ“ logs/                   # All log files
â”‚   â”œâ”€â”€ ğŸŒ server/            # Server logs
â”‚   â”œâ”€â”€ ğŸ“Š benchmark/         # Benchmark logs
â”‚   â””â”€â”€ ğŸ”§ setup/             # Setup logs
â”œâ”€â”€ âš™ï¸ config/                # Configuration files
â”œâ”€â”€ ğŸ’¾ data/                  # Data files
â”œâ”€â”€ ğŸ¤– models/                # Model files
â”‚   â””â”€â”€ nvidia--gpt-oss-120b/
â”‚       â”œâ”€â”€ ğŸ’¾ checkpoints/   # Model checkpoints
â”‚       â”œâ”€â”€ âš™ï¸ config/        # Model config
â”‚       â””â”€â”€ ğŸ”¤ tokenizer/     # Tokenizer files
â”œâ”€â”€ âš™ï¸ engines/               # TensorRT engines
â”‚   â””â”€â”€ nvidia--gpt-oss-120b/
â”‚       â”œâ”€â”€ ğŸ”¢ fp16/          # FP16 engines
â”‚       â””â”€â”€ ğŸ”¢ int8/          # INT8 engines
â””â”€â”€ ğŸ“Š benchmarks/            # Benchmark results
    â”œâ”€â”€ ğŸ“ˆ results/           # Test results
    â”œâ”€â”€ âš™ï¸ configs/           # Benchmark configs
    â””â”€â”€ ğŸ“Š plots/             # Performance plots
```

## ğŸ¤– Model Configuration

### ğŸ¯ GPT-OSS-120B Setup
- ğŸ¤– **Model:** `nvidia/gpt-oss-120b`
- ğŸ”¢ **Precision:** FP16 (optimized for B200 GPUs)
- ğŸ”— **Tensor Parallelism:** 8 GPUs
- ğŸ§  **Paged Attention:** Enabled for memory efficiency
- ğŸ“ **Max Sequence Length:** 4096 tokens
- ğŸ’¾ **Storage:** ~183GB downloaded model

### ğŸ¨ Supported Models
**TensorRT-LLM supports major model architectures:**
- ğŸ§  GPT-style models (GPT-2, GPT-J, GPT-NeoX)
- ğŸ¦™ LLaMA models (LLaMA 2, LLaMA 3, Code Llama)
- ğŸ¦… Falcon models
- ğŸ¦ MPT models
- â• And more...

**Full list:** [TensorRT-LLM Supported Models](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples)

### â¬‡ï¸ Model Download Process

**â­ Automatic (Recommended):**
- ğŸ¤– TensorRT-LLM automatically downloads models from Hugging Face during engine building
- ğŸ’¾ Models are cached at `~/.cache/huggingface/hub/`
- ğŸ”„ Subsequent builds use the cached version

**ğŸ”§ Manual Pre-download:**
```bash
# Inside the container
export MODEL="nvidia/gpt-oss-120b"
bash /workspace/download_model.sh
```

### â±ï¸ Download Time Estimates:
- ğŸ¤– **GPT-OSS-120B:** 30-60 minutes (183GB)
- ğŸ¦™ **Llama-3-70B:** 15-30 minutes
- ğŸ¦™ **Llama-2-7B:** 5-10 minutes

## âš¡ Performance Tuning

### ğŸ”§ TensorRT Engine Configuration
```bash
# Inside container - GPT-OSS-120B optimized settings
export MODEL_NAME="nvidia/gpt-oss-120b"
export TENSOR_PARALLEL_SIZE=8
export PIPELINE_PARALLEL_SIZE=1
export PRECISION="float16"
export MAX_BATCH_SIZE=8
export MAX_INPUT_LEN=4096
export MAX_OUTPUT_LEN=1024

# Build engine with paged attention
trtllm-build --checkpoint_dir /workspace/models/${MODEL_NAME} \
             --output_dir /workspace/engines/${MODEL_NAME} \
             --gemm_plugin float16 \
             --paged_kv_cache enable \
             --max_batch_size ${MAX_BATCH_SIZE} \
             --max_input_len ${MAX_INPUT_LEN} \
             --max_output_len ${MAX_OUTPUT_LEN}
```

### ğŸš€ Multi-GPU Optimization
- ğŸ”— **Tensor Parallelism:** Distributes model weights across 8 GPUs
- ğŸ“¦ **Pipeline Parallelism:** Splits model layers across GPUs
- ğŸ§  **Paged KV Cache:** Efficient memory management for long contexts
- ğŸ”„ **In-flight Batching:** Concurrent request processing

## Container Management

### List Containers
```bash
enroot list
```

### Remove Container
```bash
enroot remove tensorrt-llm
```

### Recreate Container
```bash
bash deploy/02_setup_container.sh
```

## ğŸ“Š Benchmarking

**Run benchmarks after starting the server:**

```bash
# Quick server test (recommended first)
bash scripts/test/quick_test_server.sh

# Full comprehensive benchmark
python3 scripts/benchmark/benchmark_tensorrt_llm.py
```

**This will test:**
- ğŸš€ **Token generation throughput**
- âš¡ **Latency measurements**
- ğŸ”„ **Concurrent request handling**
- ğŸ’¾ **Memory utilization**

### ğŸ”¬ Advanced Benchmarking

**For production benchmarking, use NVIDIA tools:**

- ğŸ“ˆ **GenAI Perf Analyzer:** Inference server performance
- ğŸ§® **TensorRT-LLM Bench:** Engine-level performance metrics

**ğŸ“š References:**
- [TensorRT-LLM Benchmarking](https://github.com/NVIDIA/TensorRT-LLM/tree/main/benchmarks)
- [GenAI Perf Documentation](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/perf_analyzer/genai-perf/README.html)

## ğŸ” Troubleshooting

### ğŸ³ Container Creation Issues

```bash
# Check NGC credentials
cat ~/.config/enroot/.credentials

# Verify NGC API key
curl -H "Authorization: Bearer YOUR_API_KEY" https://api.ngc.nvidia.com/v2/org/nvidia/containers/tensorrt_llm
```

### ğŸ® GPU Not Detected Inside Container

```bash
# Verify GPUs are visible
nvidia-smi

# Check CUDA installation
nvcc --version
python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"
```

### ğŸ”§ Engine Building Fails

```bash
# Check available disk space (need ~400GB for engine)
df -h /workspace

# Verify model download completed
ls -la /workspace/models/nvidia/gpt-oss-120b/

# Check TensorRT-LLM installation
python3 -c "import tensorrt_llm; print(tensorrt_llm.__version__)"
```

### ğŸ’¾ Out of Memory During Engine Build

```bash
# Reduce batch size or sequence length
export MAX_BATCH_SIZE=4
export MAX_INPUT_LEN=2048
export MAX_OUTPUT_LEN=512

# Or use fewer GPUs for building (then load on all 8)
export TENSOR_PARALLEL_SIZE=4
```

### ğŸš« Server Won't Start

```bash
# Check if port is already in use
netstat -tuln | grep 8000

# Use a different port
export PORT=8001
python3 -m tensorrt_llm.server --model_path /workspace/engines/nvidia/gpt-oss-120b --port ${PORT}
```

## Files

| File | Purpose |
|------|---------|
| `01_verify_hardware.sh` | Verify 8x B200 GPUs and NVSwitch |
| `02_setup_container.sh` | Setup Enroot container with NGC auth |
| `03_start_container.sh` | Start container with GPU access |
| `04_deploy_tensorrt_llm.sh` | Build TensorRT engine and start server |
| `05_test_server.sh` | Test the running server |
| `download_model.sh` | Pre-download models |
| `benchmark_tensorrt_llm.py` | Comprehensive performance benchmarking |
| `quick_test_server.sh` | Quick server health check |
| `install_benchmark_deps.sh` | Install benchmarking dependencies |
| `requirements_benchmark.txt` | Python dependencies for benchmarking |
| `BENCHMARK_README.md` | Detailed benchmarking guide |

## Workflow Summary

### One-Time Setup (Do This First)
```bash
# Setup both containers (recommended)
bash scripts/setup/setup_both_frameworks.sh

# Or setup individually
bash deploy/02_setup_container.sh tensorrt-llm
bash deploy/02_setup_container.sh vllm
```

## ğŸ’» API Usage Examples

### ğŸ Python Client

```python
import requests

response = requests.post(
    "http://localhost:8000/v1/completions",
    json={
        "model": "nvidia/gpt-oss-120b",
        "prompt": "Write a Python function to calculate fibonacci:",
        "max_tokens": 200,
        "temperature": 0.7
    }
)

print(response.json()["choices"][0]["text"])
```

### ğŸŒ cURL

```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nvidia/gpt-oss-120b",
    "prompt": "Hello, how are you?",
    "max_tokens": 50
  }'
```

## ğŸ¤ Contributing

**We welcome contributions!** ğŸš€

1. ğŸ´ **Fork** the repository
2. ğŸŒ¿ **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ”§ **Make** your changes
4. âœ… **Test** thoroughly on B200 hardware
5. ğŸ“¤ **Submit** a pull request

### ğŸ“‹ Development Guidelines
- ğŸ§ª Test all changes on actual B200 hardware
- ğŸ“š Update documentation for any new features
- ğŸ”„ Follow existing code style and patterns
- ğŸ·ï¸ Use clear commit messages

## ğŸ“„ License

**This project is licensed under the Apache 2.0 License** - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Authors



## ğŸ†˜ Support

**For issues and questions:**
- ğŸ› **Create an issue** in this repository
- ğŸ‘¨â€ğŸ’¼ **Contact** the cluster administrators
- ğŸ“– **Check** NVIDIA documentation for TensorRT-LLM and vLLM

---

<div align="center">

**Made with â¤ï¸ by the NVIDIA AI Team**

[ğŸ“– TensorRT-LLM Docs](https://github.com/NVIDIA/TensorRT-LLM) â€¢ [ğŸŒ NGC Catalog](https://catalog.ngc.nvidia.com/containers) â€¢ [ğŸ¤– Supported Models](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples)

</div>
